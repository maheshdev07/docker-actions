"""
Main scraper module for docker-actions GST portal scraper
"""
import sys
import requests
from bs4 import BeautifulSoup
from loguru import logger

from config import (
    GST_SEARCH_URL,
    GST_TAXPAYER_URL,
    DELAY_BETWEEN_REQUESTS,
    MAX_RETRIES,
    REQUEST_TIMEOUT,
    OUTPUT_FORMAT,
    DEMO_MODE,
    SAMPLE_GSTINS
)

from utils import (
    get_headers,
    random_delay,
    validate_gstin,
    get_timestamp,
    save_to_csv,
    save_to_json,
    print_banner,
    print_summary
)


class GSTScraper:
    """
    GST Portal Web Scraper
    
    Scrapes taxpayer information from GST portal with retry logic,
    rate limiting, and comprehensive error handling.
    """
    
    def __init__(self):
        """Initialize scraper with requests session"""
        self.session = requests.Session()
        self.session.headers.update(get_headers())
        self.scraped_count = 0
        self.failed_count = 0
        
        logger.info("GST Scraper initialized")
        logger.info(f"Demo mode: {DEMO_MODE}")
        logger.info(f"Max retries: {MAX_RETRIES}")
        logger.info(f"Request timeout: {REQUEST_TIMEOUT}s")
    
    def search_gstin(self, gstin):
        """
        Search for GSTIN details
        
        Args:
            gstin (str): 15-digit GSTIN
        
        Returns:
            dict: Taxpayer information or None if failed
        """
        # Validate GSTIN format
        if not validate_gstin(gstin):
            logger.error(f"‚ùå Invalid GSTIN format: {gstin}")
            self.failed_count += 1
            return None
        
        logger.info(f"üîç Searching GSTIN: {gstin}")
        
        # Attempt scraping with retry logic
        for attempt in range(1, MAX_RETRIES + 1):
            try:
                # Make HTTP request
                response = self.session.get(
                    GST_SEARCH_URL,
                    params={'gstin': gstin},
                    timeout=REQUEST_TIMEOUT,
                    headers=get_headers()
                )
                
                # Check response status
                response.raise_for_status()
                
                # Parse HTML (this is simplified - actual GST portal requires captcha solving)
                soup = BeautifulSoup(response.content, 'lxml')
                
                # Extract data
                data = {
                    'gstin': gstin,
                    'legal_name': self._extract_field(soup, 'legalName') or 'N/A',
                    'trade_name': self._extract_field(soup, 'tradeName') or 'N/A',
                    'registration_date': self._extract_field(soup, 'registrationDate') or 'N/A',
                    'taxpayer_type': self._extract_field(soup, 'taxpayerType') or 'N/A',
                    'status': self._extract_field(soup, 'status') or 'Active',
                    'state': self._extract_field(soup, 'state') or 'N/A',
                    'center_jurisdiction': self._extract_field(soup, 'centerJurisdiction') or 'N/A',
                    'state_jurisdiction': self._extract_field(soup, 'stateJurisdiction') or 'N/A',
                    'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                    'scraper_version': '1.0'
                }
                
                logger.success(f"‚úÖ Successfully scraped: {gstin}")
                self.scraped_count += 1
                return data
                
            except requests.exceptions.Timeout:
                logger.warning(f"‚è±Ô∏è  Timeout on attempt {attempt}/{MAX_RETRIES} for {gstin}")
                if attempt < MAX_RETRIES:
                    random_delay(2, 4)
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"‚ùå Request failed on attempt {attempt}/{MAX_RETRIES}: {str(e)}")
                if attempt < MAX_RETRIES:
                    random_delay(2, 4)
                    
            except Exception as e:
                logger.error(f"‚ùå Unexpected error: {str(e)}")
                break
        
        # All retries failed
        logger.error(f"‚ùå Failed to scrape {gstin} after {MAX_RETRIES} attempts")
        self.failed_count += 1
        return None
    
    def _extract_field(self, soup, field_name):
        """
        Extract field value from HTML soup
        
        Args:
            soup (BeautifulSoup): Parsed HTML
            field_name (str): Field identifier
        
        Returns:
            str: Extracted value or None
        """
        try:
            # Try various extraction methods
            element = soup.find(id=field_name)
            if element:
                return element.text.strip()
            
            element = soup.find('span', {'class': field_name})
            if element:
                return element.text.strip()
            
            return None
        except Exception:
            return None
    
    def search_multiple_gstins(self, gstin_list):
        """
        Search multiple GSTINs with rate limiting
        
        Args:
            gstin_list (list): List of GSTINs to scrape
        
        Returns:
            list: List of scraped data dictionaries
        """
        results = []
        total = len(gstin_list)
        
        logger.info(f"üìã Starting batch scraping: {total} GSTINs")
        
        for index, gstin in enumerate(gstin_list, 1):
            logger.info(f"Progress: {index}/{total}")
            
            data = self.search_gstin(gstin)
            if data:
                results.append(data)
            
            # Add delay between requests (except last one)
            if index < total:
                random_delay(DELAY_BETWEEN_REQUESTS, DELAY_BETWEEN_REQUESTS + 1)
        
        logger.info(f"‚úÖ Batch complete: {self.scraped_count} succeeded, {self.failed_count} failed")
        return results
    
    def generate_demo_data(self):
        """
        Generate demo data for testing (when actual scraping is not possible)
        
        Returns:
            list: Demo data
        """
        logger.info("üé≠ Running in DEMO mode - generating sample data")
        
        demo_data = [
            {
                'gstin': '27AAPFU0939F1ZV',
                'legal_name': 'UBER INDIA SYSTEMS PRIVATE LIMITED',
                'trade_name': 'UBER',
                'registration_date': '01/07/2017',
                'taxpayer_type': 'Regular',
                'status': 'Active',
                'state': 'Maharashtra',
                'center_jurisdiction': 'Mumbai Central',
                'state_jurisdiction': 'Mumbai State GST',
                'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                'scraper_version': '1.0'
            },
            {
                'gstin': '29AABCT1332L1ZN',
                'legal_name': 'TATA CONSULTANCY SERVICES LIMITED',
                'trade_name': 'TCS',
                'registration_date': '01/07/2017',
                'taxpayer_type': 'Regular',
                'status': 'Active',
                'state': 'Karnataka',
                'center_jurisdiction': 'Bangalore Central',
                'state_jurisdiction': 'Karnataka State GST',
                'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                'scraper_version': '1.0'
            },
            {
                'gstin': '27AADCI7885M1ZJ',
                'legal_name': 'RELIANCE RETAIL LIMITED',
                'trade_name': 'Reliance Retail',
                'registration_date': '01/07/2017',
                'taxpayer_type': 'Regular',
                'status': 'Active',
                'state': 'Maharashtra',
                'center_jurisdiction': 'Mumbai Central',
                'state_jurisdiction': 'Mumbai State GST',
                'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                'scraper_version': '1.0'
            },
            {
                'gstin': '09AAACH7409R1ZN',
                'legal_name': 'HCL TECHNOLOGIES LIMITED',
                'trade_name': 'HCL Tech',
                'registration_date': '01/07/2017',
                'taxpayer_type': 'Regular',
                'status': 'Active',
                'state': 'Uttar Pradesh',
                'center_jurisdiction': 'Noida Central',
                'state_jurisdiction': 'UP State GST',
                'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                'scraper_version': '1.0'
            },
            {
                'gstin': '29AAACI1681G1ZU',
                'legal_name': 'INFOSYS LIMITED',
                'trade_name': 'Infosys',
                'registration_date': '01/07/2017',
                'taxpayer_type': 'Regular',
                'status': 'Active',
                'state': 'Karnataka',
                'center_jurisdiction': 'Bangalore Central',
                'state_jurisdiction': 'Karnataka State GST',
                'scraped_at': get_timestamp('%Y-%m-%d %H:%M:%S'),
                'scraper_version': '1.0'
            }
        ]
        
        self.scraped_count = len(demo_data)
        logger.success(f"‚úÖ Generated {len(demo_data)} demo records")
        
        return demo_data
    
    def save_results(self, data):
        """
        Save scraped data to files
        
        Args:
            data (list): List of scraped data dictionaries
        
        Returns:
            tuple: (csv_file_path, json_file_path)
        """
        if not data:
            logger.warning("‚ö†Ô∏è  No data to save")
            return None, None
        
        csv_file = None
        json_file = None
        
        # Save based on OUTPUT_FORMAT configuration
        if OUTPUT_FORMAT in ['csv', 'both']:
            csv_file = save_to_csv(data)
        
        if OUTPUT_FORMAT in ['json', 'both']:
            json_file = save_to_json(data)
        
        return csv_file, json_file


def main():
    """Main execution function"""
    try:
        # Print banner
        print_banner()
        
        logger.info("="*60)
        logger.info("üöÄ Starting GST Scraper")
        logger.info("="*60)
        
        # Initialize scraper
        scraper = GSTScraper()
        
        # Scrape data
        if DEMO_MODE:
            # Demo mode: Generate sample data
            data = scraper.generate_demo_data()
        else:
            # Real mode: Scrape actual GSTINs
            gstin_list = SAMPLE_GSTINS
            logger.info(f"üìù GSTINs to scrape: {gstin_list}")
            data = scraper.search_multiple_gstins(gstin_list)
        
        # Save results
        if data:
            csv_file, json_file = scraper.save_results(data)
            
            # Print summary
            print_summary(data, csv_file, json_file)
            
            logger.success("üéâ Scraping completed successfully!")
            sys.exit(0)
        else:
            logger.error("‚ùå No data was scraped")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Scraping interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.exception(f"üí• Fatal error: {str(e)}")
        sys.exit(1)
    finally:
        logger.info("="*60)
        logger.info("üèÅ GST Scraper finished")
        logger.info("="*60)


if __name__ == "__main__":
    main()
